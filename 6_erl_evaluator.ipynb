{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p97Nxnxhdndd",
        "outputId": "002948da-49ab-4d6a-cca0-3b45f0b9d31f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# For Colab/Google Drive integration:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/FinRL/final')  # Change to your project folder in Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JLKUAJgiBOd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # For non-GUI environments like Colab\n",
        "import json\n",
        "\n",
        "# ==================== IMPROVED EVALUATION PARAMETERS ====================\n",
        "# Data selection for evaluation - Choose which dataset to evaluate on\n",
        "EVAL_DATA_SPLIT = \"valid\"  # Options: \"train\", \"valid\", \"test\"\n",
        "\n",
        "# Feature toggles\n",
        "NORMALIZE_LLM_SIGNALS = True  # min-max normalize sentiment and risk\n",
        "\n",
        "REWARD_SCALE = 1e-1\n",
        "ENTROPY_COEF = 0.05\n",
        "TARGET_KL = 0.02\n",
        "\n",
        "AGENT_DIR_NAME = \"PPO\"  # Subdirectory where PPO agents are saved\n",
        "\n",
        "# ==================== NETWORK ARCHITECTURE ====================\n",
        "\n",
        "EVAL_NET_DIMS = [256, 256, 128, 128, 64] \n",
        "EVAL_STATE_DIM = 12  # 2 (position, holding) + 8 (Alpha101) + 2 (LLM signals)\n",
        "EVAL_ACTION_DIM = 3  # Short, Hold, Long\n",
        "\n",
        "# Environment\n",
        "EVAL_NUM_SIMS = 1  # single env during evaluation\n",
        "EVAL_MAX_POSITION = 1\n",
        "EVAL_STEP_GAP = 2\n",
        "EVAL_SLIPPAGE = 7e-7\n",
        "EVAL_NUM_IGNORE_STEP = 60\n",
        "\n",
        "# Evaluation settings\n",
        "EVAL_STARTING_CASH = 1e6\n",
        "EVAL_THRESH = 0.001\n",
        "\n",
        "# Device\n",
        "GPU_ID = 0\n",
        "\n",
        "# Versioning (training model directory version prefix)\n",
        "PPO_VERSION = \"5_7_3\"\n",
        "\n",
        "# ==================== PPO NETWORK ARCHITECTURE ====================\n",
        "\n",
        "class SimplePPOActor(nn.Module):\n",
        "    \"\"\"Actor network for PPO\"\"\"\n",
        "\n",
        "    def __init__(self, state_dim: int, action_dim: int, net_dims: List[int]):\n",
        "        super().__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        # Simple MLP layers\n",
        "        layers = []\n",
        "        input_dim = state_dim\n",
        "\n",
        "        for i, dim in enumerate(net_dims):\n",
        "            layers.extend([\n",
        "                nn.Linear(input_dim, dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.1)\n",
        "            ])\n",
        "            # Add layer normalization for deeper networks\n",
        "            if i < len(net_dims) - 1:\n",
        "                layers.append(nn.LayerNorm(dim))\n",
        "            input_dim = dim\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(input_dim, action_dim))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.orthogonal_(module.weight, gain=0.01)\n",
        "            nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(state)\n",
        "\n",
        "    def get_action_probs(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Get action probabilities using softmax\"\"\"\n",
        "        logits = self.forward(state)\n",
        "        return F.softmax(logits, dim=-1)\n",
        "\n",
        "    def get_action_log_probs(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Get log probabilities for specific actions - FIXED shape issues\"\"\"\n",
        "        action_probs = self.get_action_probs(state)\n",
        "        log_probs = torch.log(action_probs + 1e-8)\n",
        "        action = action.unsqueeze(-1)\n",
        "        gathered = log_probs.gather(-1, action).squeeze(-1)\n",
        "\n",
        "        return gathered\n",
        "\n",
        "class SimplePPOCritic(nn.Module):\n",
        "    \"\"\"Critic network for PPO\"\"\"\n",
        "\n",
        "    def __init__(self, state_dim: int, net_dims: List[int]):\n",
        "        super().__init__()\n",
        "        self.state_dim = state_dim\n",
        "\n",
        "        # Build MLP layers\n",
        "        layers = []\n",
        "        input_dim = state_dim\n",
        "\n",
        "        for i, dim in enumerate(net_dims):\n",
        "            layers.extend([\n",
        "                nn.Linear(input_dim, dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.1)\n",
        "            ])\n",
        "            if i < len(net_dims) - 1:\n",
        "                layers.append(nn.LayerNorm(dim))\n",
        "            input_dim = dim\n",
        "\n",
        "        # Output layer for single value\n",
        "        layers.append(nn.Linear(input_dim, 1))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.orthogonal_(module.weight, gain=1.0)\n",
        "            nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(state).squeeze(-1)\n",
        "\n",
        "# ==================== PPO AGENT ====================\n",
        "\n",
        "class ImprovedPPO:\n",
        "    \"\"\"PPO agent\"\"\"\n",
        "\n",
        "    def __init__(self, state_dim: int, action_dim: int, net_dims: List[int],\n",
        "                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        # Networks - using simplified architecture\n",
        "        self.actor = SimplePPOActor(state_dim, action_dim, net_dims).to(device)\n",
        "        self.critic = SimplePPOCritic(state_dim, net_dims).to(device)\n",
        "\n",
        "        # Training stats\n",
        "        self.total_steps = 0\n",
        "        self.episode_rewards = []\n",
        "        self.episode_lengths = []\n",
        "\n",
        "    def select_action(self, state: torch.Tensor, training: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Select action using current policy with improved exploration\"\"\"\n",
        "        with torch.no_grad():\n",
        "            action_probs = self.actor.get_action_probs(state)\n",
        "\n",
        "            if training:\n",
        "                temperature = 1.2  # Slightly higher temperature for more exploration\n",
        "                logits = self.actor(state)\n",
        "                scaled_logits = logits / temperature\n",
        "                scaled_probs = F.softmax(scaled_logits, dim=-1)\n",
        "                dist = torch.distributions.Categorical(scaled_probs)\n",
        "                action = dist.sample()\n",
        "                log_prob = dist.log_prob(action)\n",
        "            else:\n",
        "                # Greedy action selection for evaluation\n",
        "                action = torch.argmax(action_probs, dim=-1)\n",
        "                log_prob = torch.zeros_like(action, dtype=torch.float)\n",
        "\n",
        "            value = self.critic(state)\n",
        "\n",
        "        return action, log_prob, value\n",
        "\n",
        "    def save_or_load_agent(self, cwd: str, if_save: bool):\n",
        "        \"\"\"Save or load agent models\"\"\"\n",
        "        if if_save:\n",
        "            os.makedirs(cwd, exist_ok=True)\n",
        "            torch.save(self.actor.state_dict(), os.path.join(cwd, 'actor.pth'))\n",
        "            torch.save(self.critic.state_dict(), os.path.join(cwd, 'critic.pth'))\n",
        "            print(f\"💾 Saved Improved PPO agent to {cwd}\")\n",
        "        else:\n",
        "            actor_path = os.path.join(cwd, 'actor.pth')\n",
        "            critic_path = os.path.join(cwd, 'critic.pth')\n",
        "\n",
        "            if os.path.exists(actor_path) and os.path.exists(critic_path):\n",
        "                try:\n",
        "                    # Try loading as state_dict first\n",
        "                    self.actor.load_state_dict(torch.load(actor_path, map_location=self.device))\n",
        "                    self.critic.load_state_dict(torch.load(critic_path, map_location=self.device))\n",
        "                    print(f\"Loaded Improved PPO agent from {cwd}\")\n",
        "                    return True\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading Improved PPO agent: {e}\")\n",
        "                    print(f\"Debug: Let's inspect the saved model structure...\")\n",
        "\n",
        "                    try:\n",
        "                        actor_state = torch.load(actor_path, map_location=self.device)\n",
        "                        critic_state = torch.load(critic_path, map_location=self.device)\n",
        "\n",
        "                        print(f\"Saved Actor keys: {list(actor_state.keys())}\")\n",
        "                        print(f\"Saved Critic keys: {list(critic_state.keys())}\")\n",
        "                        print(f\"Current Actor keys: {list(self.actor.state_dict().keys())}\")\n",
        "                        print(f\"Current Critic keys: {list(self.critic.state_dict().keys())}\")\n",
        "\n",
        "                        # Try loading with strict=False\n",
        "                        missing_keys, unexpected_keys = self.actor.load_state_dict(actor_state, strict=False)\n",
        "                        missing_keys2, unexpected_keys2 = self.critic.load_state_dict(critic_state, strict=False)\n",
        "\n",
        "                        print(f\"Actor missing keys: {missing_keys}\")\n",
        "                        print(f\"Actor unexpected keys: {unexpected_keys}\")\n",
        "                        print(f\"Critic missing keys: {missing_keys2}\")\n",
        "                        print(f\"Critic unexpected keys: {unexpected_keys2}\")\n",
        "\n",
        "                        if len(missing_keys) == 0 and len(missing_keys2) == 0:\n",
        "                            print(f\"Loaded Improved PPO agent with strict=False from {cwd}\")\n",
        "                            return True\n",
        "                        else:\n",
        "                            print(f\"Still missing keys even with strict=False\")\n",
        "                            return False\n",
        "\n",
        "                    except Exception as e2:\n",
        "                        print(f\"Error during debug loading: {e2}\")\n",
        "                        return False\n",
        "            else:\n",
        "                print(f\"Model files not found in {cwd}\")\n",
        "                return False\n",
        "\n",
        "# ==================== TRADING ENVIRONMENT ====================\n",
        "\n",
        "class TradeSimulator:\n",
        "    def __init__(self, num_sims=1, slippage=5e-5, max_position=1, step_gap=1,\n",
        "                 delay_step=1, num_ignore_step=60, device=torch.device(\"cpu\"), gpu_id=-1, timeframe='1sec'):\n",
        "        self.device = torch.device(f\"cuda:{gpu_id}\") if gpu_id >= 0 else device\n",
        "        self.num_sims = num_sims\n",
        "        self.slippage = slippage\n",
        "        self.delay_step = delay_step\n",
        "        self.max_holding = 60 * 60 // step_gap\n",
        "        self.max_position = max_position\n",
        "        self.step_gap = step_gap\n",
        "        self.sim_ids = torch.arange(self.num_sims, device=self.device)\n",
        "        self.timeframe = timeframe\n",
        "\n",
        "        self.load_data()\n",
        "\n",
        "        self.env_name = \"TradeSimulator-v0\"\n",
        "        self.state_dim = 2 + 8 + 2\n",
        "        self.action_dim = 3\n",
        "        self.if_discrete = True\n",
        "        self.max_step = (self.seq_len - num_ignore_step) // step_gap\n",
        "        self.target_return = +np.inf\n",
        "\n",
        "        self.best_price = torch.zeros((num_sims,), dtype=torch.float32, device=device)\n",
        "        self.stop_loss_thresh = 1e-3\n",
        "\n",
        "    def load_data(self):\n",
        "        # Load data based on evaluation configuration with multiple fallback options\n",
        "        possible_paths = [\n",
        "            f\"output/{self.timeframe}/{EVAL_DATA_SPLIT}_predictions.npy\",\n",
        "            f\"output/{self.timeframe}/train_predictions.npy\",\n",
        "            f\"output/{self.timeframe}/predictions.npy\",\n",
        "            f\"../output/{self.timeframe}/{EVAL_DATA_SPLIT}_predictions.npy\",\n",
        "            f\"../output/{self.timeframe}/train_predictions.npy\",\n",
        "            f\"../output/{self.timeframe}/predictions.npy\"\n",
        "        ]\n",
        "\n",
        "        factor_path = None\n",
        "        data_type = EVAL_DATA_SPLIT\n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                factor_path = path\n",
        "                print(f\"Found {data_type} data at: {path}\")\n",
        "                break\n",
        "\n",
        "        if factor_path is None:\n",
        "            # Quit evaluation if no predictions found\n",
        "            print(\"Error: No prediction files found in any of the expected locations:\")\n",
        "            for path in possible_paths:\n",
        "                print(f\"   - {path}\")\n",
        "            print(\"Please ensure the prediction files exist before running evaluation.\")\n",
        "            raise FileNotFoundError(\"Required prediction files not found. Cannot proceed with evaluation.\")\n",
        "\n",
        "        self.factor_ary = np.load(factor_path)\n",
        "        self.factor_ary = torch.tensor(self.factor_ary, dtype=torch.float32)\n",
        "\n",
        "        print(f\"Loaded {data_type} data: {self.factor_ary.shape}\")\n",
        "\n",
        "        # Data leakage warning\n",
        "        if EVAL_DATA_SPLIT == \"train\":\n",
        "            print(\"WARNING: Evaluating on training data - this will give overly optimistic results!\")\n",
        "            print(\"Consider setting EVAL_DATA_SPLIT to 'valid' or 'test' for proper evaluation.\")\n",
        "\n",
        "        if self.factor_ary.shape[0] == 0:\n",
        "            raise ValueError(f\"Alpha101 factors are empty for {self.timeframe}.\")\n",
        "\n",
        "        # Try multiple paths for price data\n",
        "        possible_csv_paths = [\n",
        "            f\"data/{self.timeframe}/BTC_{self.timeframe}_with_sentiment_risk_train_{self.timeframe}_train_70.csv\",\n",
        "            f\"../data/{self.timeframe}/BTC_{self.timeframe}_with_sentiment_risk_train_{self.timeframe}_train_70.csv\",\n",
        "            f\"data/{self.timeframe}/BTC_{self.timeframe}_with_sentiment_risk_train.csv\",\n",
        "            f\"../data/{self.timeframe}/BTC_{self.timeframe}_with_sentiment_risk_train.csv\"\n",
        "        ]\n",
        "\n",
        "        csv_path = None\n",
        "        for path in possible_csv_paths:\n",
        "            if os.path.exists(path):\n",
        "                csv_path = path\n",
        "                print(f\"Found price data at: {path}\")\n",
        "                break\n",
        "\n",
        "        if csv_path is None:\n",
        "            # Quit evaluation if no price data found\n",
        "            print(\"Error: No price data files found in any of the expected locations:\")\n",
        "            for path in possible_csv_paths:\n",
        "                print(f\"   - {path}\")\n",
        "            print(\"Please ensure the price data files exist before running evaluation.\")\n",
        "            raise FileNotFoundError(\"Required price data files not found. Cannot proceed with evaluation.\")\n",
        "        try:\n",
        "            data_df = pd.read_csv(csv_path, engine='python')\n",
        "        except Exception:\n",
        "            data_df = pd.read_csv(csv_path)\n",
        "\n",
        "        required_columns = [\"bids_distance_3\", \"asks_distance_3\", \"midpoint\", \"sentiment_score\", \"risk_score\"]\n",
        "        missing_columns = [col for col in required_columns if col not in data_df.columns]\n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"Missing required columns: {missing_columns}.\")\n",
        "\n",
        "        self.price_ary = data_df[[\"bids_distance_3\", \"asks_distance_3\", \"midpoint\"]].values\n",
        "        self.price_ary[:, 0] = self.price_ary[:, 2] * (1 + self.price_ary[:, 0])\n",
        "        self.price_ary[:, 1] = self.price_ary[:, 2] * (1 + self.price_ary[:, 0])\n",
        "        self.llm_signals = data_df[[\"sentiment_score\", \"risk_score\"]].values\n",
        "\n",
        "        min_len = min(self.factor_ary.shape[0], self.price_ary.shape[0], self.llm_signals.shape[0])\n",
        "        self.factor_ary = self.factor_ary[:min_len]\n",
        "        self.price_ary = torch.tensor(self.price_ary[:min_len], dtype=torch.float32)\n",
        "        self.llm_signals = torch.tensor(self.llm_signals[:min_len], dtype=torch.float32)\n",
        "\n",
        "        if NORMALIZE_LLM_SIGNALS:\n",
        "            llm_min = self.llm_signals.amin(dim=0, keepdim=True)\n",
        "            llm_max = self.llm_signals.amax(dim=0, keepdim=True)\n",
        "            llm_range = (llm_max - llm_min)\n",
        "            self.llm_signals = (self.llm_signals - llm_min) / (llm_range + 1e-6)\n",
        "            self.llm_signals = torch.clamp(self.llm_signals, 0.0, 1.0)\n",
        "\n",
        "        if self.timeframe == '1sec':\n",
        "            self.seq_len = 1800\n",
        "        elif self.timeframe == '1min':\n",
        "            self.seq_len = 64\n",
        "        elif self.timeframe == '5min':\n",
        "            self.seq_len = 16\n",
        "        else:\n",
        "            self.seq_len = 1800\n",
        "\n",
        "        self.full_seq_len = self.price_ary.shape[0]\n",
        "\n",
        "    def reset(self, slippage=None):\n",
        "        self.slippage = slippage if isinstance(slippage, float) else self.slippage\n",
        "\n",
        "        min_start = self.seq_len\n",
        "        max_start = self.full_seq_len - self.seq_len * 2\n",
        "        if min_start >= max_start:\n",
        "            if self.full_seq_len > self.seq_len:\n",
        "                max_start = self.full_seq_len - self.seq_len\n",
        "                min_start = 0\n",
        "                i0s = np.random.randint(min_start, max_start, size=self.num_sims)\n",
        "            else:\n",
        "                if self.full_seq_len > 0:\n",
        "                    i0s = np.arange(min(self.num_sims, self.full_seq_len))\n",
        "                    if len(i0s) < self.num_sims:\n",
        "                        i0s = np.pad(i0s, (0, self.num_sims - len(i0s)), mode='constant', constant_values=0)\n",
        "                else:\n",
        "                    i0s = np.zeros(self.num_sims, dtype=np.int64)\n",
        "        else:\n",
        "            i0s = np.random.randint(min_start, max_start, size=self.num_sims)\n",
        "\n",
        "        self.step_i = 0\n",
        "        self.step_is = torch.tensor(i0s, dtype=torch.long, device=self.device)\n",
        "        # Initialize with starting cash instead of zero\n",
        "        starting_cash = 1000000.0  # $1M starting capital\n",
        "        self.cash = torch.full((self.num_sims,), starting_cash, dtype=torch.float32, device=self.device)\n",
        "        self.asset = torch.full((self.num_sims,), starting_cash, dtype=torch.float32, device=self.device)\n",
        "\n",
        "        # Debug logging for asset initialization\n",
        "        print(f\"Debug: Environment reset - Cash: {self.cash[0].item():.2f}, Asset: {self.asset[0].item():.2f}\")\n",
        "        self.holding = torch.zeros((self.num_sims,), dtype=torch.long, device=self.device)\n",
        "        self.position = torch.zeros((self.num_sims,), dtype=torch.long, device=self.device)\n",
        "        self.empty_count = torch.zeros((self.num_sims,), dtype=torch.long, device=self.device)\n",
        "        self.best_price = torch.zeros((self.num_sims,), dtype=torch.float32, device=self.device)\n",
        "\n",
        "        step_is = self.step_is + self.step_i\n",
        "        state = self.get_state(step_is)\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        try:\n",
        "            if not hasattr(self, 'step_i'):\n",
        "                self.step_i = 0\n",
        "            self.step_i += self.step_gap\n",
        "            step_is = self.step_is + self.step_i\n",
        "\n",
        "            if action.dim() == 1:\n",
        "                action = action.to(self.device)\n",
        "            else:\n",
        "                action = action.squeeze().to(self.device)\n",
        "            action_int = action - 1\n",
        "\n",
        "            old_cash = self.cash\n",
        "            old_asset = self.asset\n",
        "            old_position = self.position\n",
        "\n",
        "            step_is_cpu = step_is.cpu()\n",
        "            max_idx = self.price_ary.shape[0] - 1\n",
        "            step_is_cpu = torch.clamp(step_is_cpu, 0, max_idx)\n",
        "            mid_price = self.price_ary[step_is_cpu, 2].to(self.device)\n",
        "\n",
        "            truncated = self.step_i >= (self.max_step * self.step_gap)\n",
        "            if truncated:\n",
        "                action_int = -old_position\n",
        "            else:\n",
        "                new_position = (old_position + action_int).clip(-self.max_position, self.max_position)\n",
        "                action_int = new_position - old_position\n",
        "\n",
        "            self.holding = self.holding + 1\n",
        "            mask_max_holding = self.holding.gt(self.max_holding)\n",
        "            if mask_max_holding.sum() > 0:\n",
        "                action_int[mask_max_holding] = -old_position[mask_max_holding]\n",
        "            self.holding[old_position == 0] = 0\n",
        "\n",
        "            direction_mask1 = old_position.gt(0)\n",
        "            if direction_mask1.sum() > 0:\n",
        "                _best_price = torch.max(\n",
        "                    torch.stack([self.best_price[direction_mask1], mid_price[direction_mask1]]),\n",
        "                    dim=0,\n",
        "                )[0]\n",
        "                self.best_price[direction_mask1] = _best_price\n",
        "\n",
        "            direction_mask2 = old_position.lt(0)\n",
        "            if direction_mask2.sum() > 0:\n",
        "                _best_price = torch.min(\n",
        "                    torch.stack([self.best_price[direction_mask2], mid_price[direction_mask2]]),\n",
        "                    dim=0,\n",
        "                )[0]\n",
        "                self.best_price[direction_mask2] = _best_price\n",
        "\n",
        "            stop_loss_mask1 = torch.logical_and(direction_mask1, (self.best_price - mid_price).gt(self.stop_loss_thresh))\n",
        "            stop_loss_mask2 = torch.logical_and(direction_mask2, (mid_price - self.best_price).gt(self.stop_loss_thresh))\n",
        "            stop_loss_mask = torch.logical_or(stop_loss_mask1, stop_loss_mask2)\n",
        "            if stop_loss_mask.sum() > 0:\n",
        "                action_int[stop_loss_mask] = -old_position[stop_loss_mask]\n",
        "\n",
        "            new_position = old_position + action_int\n",
        "            direction = action_int.gt(0)\n",
        "            cost = action_int * mid_price\n",
        "            new_cash = old_cash - cost * torch.where(direction, 1 + self.slippage, 1 - self.slippage)\n",
        "            new_asset = new_cash + new_position * mid_price\n",
        "            reward = new_asset - old_asset\n",
        "\n",
        "            # Debug logging for asset changes\n",
        "            if self.step_i % 100 == 0:  # Debug every 100 steps\n",
        "                print(f\"Debug: Step {self.step_i} - Old Asset: {old_asset[0].item():.2f}, \"\n",
        "                      f\"New Asset: {new_asset[0].item():.2f}, Reward: {reward[0].item():.6f}\")\n",
        "\n",
        "            self.cash = new_cash\n",
        "            self.asset = new_asset\n",
        "            self.position = new_position\n",
        "\n",
        "            state = self.get_state(step_is)\n",
        "            if truncated:\n",
        "                terminal = torch.ones_like(self.position, dtype=torch.bool)\n",
        "                # Don't reset during evaluation - let caller handle episode termination\n",
        "                # state = self.reset()  # Commented out to prevent evaluation resets\n",
        "            else:\n",
        "                terminal = torch.zeros_like(self.position, dtype=torch.bool)\n",
        "\n",
        "            return state, reward, terminal, {}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in environment step: {e}\")\n",
        "            state = torch.zeros((self.num_sims, self.state_dim), dtype=torch.float32).to(self.device)\n",
        "            reward = torch.zeros((self.num_sims,), dtype=torch.float32).to(self.device)\n",
        "            terminal = torch.zeros((self.num_sims,), dtype=torch.bool).to(self.device)\n",
        "            return state, reward, terminal, {}\n",
        "\n",
        "    def get_state(self, step_is):\n",
        "        step_is_cpu = step_is.cpu()\n",
        "        max_idx = min(self.factor_ary.shape[0], self.price_ary.shape[0], self.llm_signals.shape[0]) - 1\n",
        "        step_is_cpu = torch.clamp(step_is_cpu, 0, max_idx)\n",
        "        factor_ary = self.factor_ary[step_is_cpu, :].to(self.device)\n",
        "        llm_signals = self.llm_signals[step_is_cpu, :].to(self.device)\n",
        "        return torch.concat(\n",
        "            (\n",
        "                (self.position.float() / self.max_position)[:, None],\n",
        "                (self.holding.float() / self.max_holding)[:, None],\n",
        "                factor_ary,\n",
        "                llm_signals,\n",
        "            ),\n",
        "            dim=1,\n",
        "        )\n",
        "\n",
        "# ==================== FINANCIAL METRICS ====================\n",
        "\n",
        "def cumulative_returns(returns_pct):\n",
        "    \"\"\"Calculate cumulative returns with safe handling\"\"\"\n",
        "    if isinstance(returns_pct, np.ndarray):\n",
        "        return np.cumprod(1 + returns_pct)\n",
        "    else:\n",
        "        return (1 + returns_pct).cumprod()\n",
        "\n",
        "def sharpe_ratio(returns_pct, risk_free=0):\n",
        "    \"\"\"Calculate Sharpe ratio with safe handling\"\"\"\n",
        "    returns = np.array(returns_pct)\n",
        "    if len(returns) == 0:\n",
        "        return 0.0\n",
        "    if returns.std() == 0:\n",
        "        sharpe_ratio = np.inf\n",
        "    else:\n",
        "        sharpe_ratio = (returns.mean()-risk_free) / returns.std()\n",
        "    return sharpe_ratio\n",
        "\n",
        "def max_drawdown(returns_pct):\n",
        "    \"\"\"Calculate max drawdown with safe division\"\"\"\n",
        "    if len(returns_pct) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    cumulative = cumulative_returns(returns_pct)\n",
        "    if isinstance(cumulative, np.ndarray):\n",
        "        running_max = np.maximum.accumulate(cumulative)\n",
        "        running_max_safe = np.where(running_max == 0, 1e-8, running_max)\n",
        "        drawdown = (cumulative - running_max) / running_max_safe\n",
        "        return drawdown.min()\n",
        "    else:\n",
        "        running_max = cumulative.expanding().max()\n",
        "        running_max_safe = running_max.replace(0, 1e-8)\n",
        "        drawdown = (cumulative - running_max) / running_max_safe\n",
        "        return drawdown.min()\n",
        "\n",
        "def return_over_max_drawdown(returns_pct):\n",
        "    \"\"\"Calculate return over max drawdown with safe handling\"\"\"\n",
        "    if len(returns_pct) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    mdd = abs(max_drawdown(returns_pct))\n",
        "    cumulative = cumulative_returns(returns_pct)\n",
        "\n",
        "    if isinstance(cumulative, np.ndarray):\n",
        "        total_return = cumulative[-1] - 1\n",
        "    else:\n",
        "        total_return = cumulative.iloc[-1] - 1\n",
        "\n",
        "    return total_return / mdd if mdd > 0 else 0.0\n",
        "\n",
        "# ==================== IMPROVED ENSEMBLE EVALUATOR ====================\n",
        "\n",
        "class ImprovedEnsembleEvaluator:\n",
        "    \"\"\"Ensemble Evaluator\"\"\"\n",
        "\n",
        "    def __init__(self, save_path: str, agent_classes: List, timeframe: str = '1sec', gpu_id: int = 0):\n",
        "        self.save_path = save_path\n",
        "        self.agent_classes = agent_classes\n",
        "        self.timeframe = timeframe\n",
        "        self.gpu_id = gpu_id\n",
        "        self.device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Initialize environment with same parameters as training\n",
        "        self.trade_env = TradeSimulator(\n",
        "            num_sims=EVAL_NUM_SIMS,\n",
        "            slippage=EVAL_SLIPPAGE,\n",
        "            max_position=EVAL_MAX_POSITION,\n",
        "            step_gap=EVAL_STEP_GAP,\n",
        "            gpu_id=gpu_id,\n",
        "            timeframe=timeframe\n",
        "        )\n",
        "\n",
        "        # Debug environment info\n",
        "        print(f\"Environment initialized:\")\n",
        "        print(f\"   - max_step: {self.trade_env.max_step}\")\n",
        "        print(f\"   - seq_len: {self.trade_env.seq_len}\")\n",
        "        print(f\"   - full_seq_len: {self.trade_env.full_seq_len}\")\n",
        "        print(f\"   - state_dim: {self.trade_env.state_dim}\")\n",
        "        print(f\"   - action_dim: {self.trade_env.action_dim}\")\n",
        "\n",
        "        # Initialize agents\n",
        "        self.agents = []\n",
        "        self.agent_paths = []\n",
        "\n",
        "        # Set starting cash\n",
        "        self.starting_cash = EVAL_STARTING_CASH\n",
        "\n",
        "        # Results storage\n",
        "        self.net_assets = []\n",
        "        self.positions = []\n",
        "        self.cash = []\n",
        "        self.btc_positions = []\n",
        "        self.midpoints = []\n",
        "        self.cum_returns = []\n",
        "\n",
        "    def load_agents(self):\n",
        "        \"\"\"Load trained agents from save_path\"\"\"\n",
        "        print(f\"Loading agents from: {self.save_path}\")\n",
        "\n",
        "        # For PPO, we expect actor.pth and critic.pth files directly in save_path\n",
        "        actor_path = os.path.join(self.save_path, \"actor.pth\")\n",
        "        critic_path = os.path.join(self.save_path, \"critic.pth\")\n",
        "\n",
        "        if os.path.exists(actor_path) and os.path.exists(critic_path):\n",
        "            # Create agent instance with correct state dimension from environment\n",
        "            agent = ImprovedPPO(\n",
        "                state_dim=self.trade_env.state_dim,\n",
        "                action_dim=self.trade_env.action_dim,\n",
        "                net_dims=EVAL_NET_DIMS,\n",
        "                device=self.device\n",
        "            )\n",
        "\n",
        "            # Load trained weights directly from save_path\n",
        "            if agent.save_or_load_agent(self.save_path, if_save=False):\n",
        "                # Check if agent is already loaded to prevent duplicates\n",
        "                if not any(agent_path == self.save_path for agent_path in self.agent_paths):\n",
        "                    self.agents.append(agent)\n",
        "                    self.agent_paths.append(self.save_path)\n",
        "                    print(f\"Loaded PPO agent from {self.save_path}\")\n",
        "                else:\n",
        "                    print(f\"PPO agent already loaded from {self.save_path}, skipping duplicate\")\n",
        "            else:\n",
        "                print(f\"Failed to load PPO agent from {self.save_path}\")\n",
        "        else:\n",
        "            print(f\"PPO model files not found in {self.save_path}\")\n",
        "            print(f\"Expected: {actor_path}\")\n",
        "            print(f\"Expected: {critic_path}\")\n",
        "            if os.path.exists(self.save_path):\n",
        "                print(f\"  Available files in {self.save_path}:\")\n",
        "                for item in os.listdir(self.save_path):\n",
        "                    item_path = os.path.join(self.save_path, item)\n",
        "                    if os.path.isdir(item_path):\n",
        "                        print(f\"{item}/\")\n",
        "                    else:\n",
        "                        print(f\"{item}\")\n",
        "\n",
        "        if not self.agents:\n",
        "            raise ValueError(\"No agents loaded successfully!\")\n",
        "\n",
        "        print(f\"Loaded {len(self.agents)} agents successfully\")\n",
        "        print(f\"Agent details:\")\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            print(f\"Agent {i}: {type(agent).__name__}\")\n",
        "            print(f\"State dim: {agent.state_dim}\")\n",
        "            print(f\"Action dim: {agent.action_dim}\")\n",
        "\n",
        "    def multi_trade(self) -> dict:\n",
        "        \"\"\"Run improved multi-agent trading simulation\"\"\"\n",
        "        print(f\"Starting multi-agent trading simulation...\")\n",
        "\n",
        "        # Initialize environment\n",
        "        initial_state = self.trade_env.reset()\n",
        "\n",
        "        # Initialize arrays for tracking\n",
        "        num_steps = self.trade_env.max_step\n",
        "        num_agents = len(self.agents)\n",
        "\n",
        "        # Debug information\n",
        "        print(f\"Debug: num_steps={num_steps}, num_agents={num_agents}\")\n",
        "\n",
        "        # Ensure num_steps is positive\n",
        "        if num_steps <= 0:\n",
        "            num_steps = 1000  # Fallback value\n",
        "            print(f\"Warning: max_step was {self.trade_env.max_step}, using fallback {num_steps}\")\n",
        "\n",
        "        # Ensure num_agents is positive\n",
        "        if num_agents <= 0:\n",
        "            print(f\"Error: No agents available for evaluation!\")\n",
        "            raise ValueError(f\"Expected at least 1 agent, but got {num_agents}\")\n",
        "\n",
        "        print(f\"Debug: Creating arrays with shape ({num_steps}, {num_agents})\")\n",
        "        self.net_assets = np.zeros((num_steps, num_agents))\n",
        "        self.positions = np.zeros((num_steps, num_agents))\n",
        "        self.cash = np.zeros((num_steps, num_agents))\n",
        "        self.btc_positions = np.zeros((num_steps, num_agents))\n",
        "        self.midpoints = []\n",
        "\n",
        "        print(f\"Debug: Array shapes - net_assets: {self.net_assets.shape}, positions: {self.positions.shape}\")\n",
        "\n",
        "        # Initialize starting values\n",
        "        for i in range(num_agents):\n",
        "            self.net_assets[0, i] = self.starting_cash\n",
        "            self.cash[0, i] = self.starting_cash\n",
        "            self.btc_positions[0, i] = 0\n",
        "\n",
        "        #Debug logging for initial setup\n",
        "        print(f\"🔍 Debug: Initial setup complete. Starting cash: ${self.starting_cash}\")\n",
        "        print(f\"🔍 Debug: Net assets[0]: {self.net_assets[0]}\")\n",
        "\n",
        "        # Trading loop\n",
        "        step = 0\n",
        "\n",
        "        try:\n",
        "            while step < num_steps - 1:\n",
        "                # Get current state (first simulation)\n",
        "                state = self.trade_env.get_state(self.trade_env.step_is)\n",
        "                state_single = state[0:1]  # Take first simulation for single agent evaluation\n",
        "\n",
        "                # Collect actions from all agents\n",
        "                actions = []\n",
        "                for agent in self.agents:\n",
        "                    action, _, _ = agent.select_action(state_single, training=False)\n",
        "                    actions.append(action.item())\n",
        "\n",
        "                # Majority voting for ensemble decision\n",
        "                action_counts = Counter(actions)\n",
        "                ensemble_action = action_counts.most_common(1)[0][0]\n",
        "\n",
        "                # Convert action from [0,1,2] to [1,2,3] for environment\n",
        "                action_env = torch.tensor([ensemble_action + 1], dtype=torch.long, device=self.trade_env.device).expand(self.trade_env.num_sims)\n",
        "\n",
        "                # Execute ensemble action\n",
        "                next_state, reward, terminal, info = self.trade_env.step(action_env)\n",
        "\n",
        "                #Apply reward scaling and shaping\n",
        "                base_reward = reward[0].item() * REWARD_SCALE\n",
        "\n",
        "                # Reward shaping to encourage trading\n",
        "                current_position = self.trade_env.position[0].item()\n",
        "                prev_position = getattr(self.trade_env, '_prev_position', 0)\n",
        "                position_change = abs(current_position - prev_position)\n",
        "                trading_bonus = 0.001 * position_change  # Small bonus for taking positions\n",
        "\n",
        "                # Profit incentive\n",
        "                profit_incentive = 0.01 * base_reward if base_reward > 0 else 0\n",
        "\n",
        "                shaped_reward = base_reward + trading_bonus + profit_incentive\n",
        "\n",
        "                # Store previous position for next step\n",
        "                self.trade_env._prev_position = current_position\n",
        "\n",
        "                # Check terminal condition BEFORE updating arrays\n",
        "                # This prevents using reset values from the environment\n",
        "                if terminal[0].item():\n",
        "                    print(f\"🔍 Debug: Episode terminated at step {step}\")\n",
        "                    break\n",
        "\n",
        "                # Update tracking arrays - only if episode hasn't terminated\n",
        "                if step < num_steps - 1:\n",
        "                    for i in range(num_agents):\n",
        "                        # Update net assets (use first simulation)\n",
        "                        current_asset = self.trade_env.asset[0].item()\n",
        "                        self.net_assets[step + 1, i] = current_asset\n",
        "\n",
        "                        # Update positions (use first simulation)\n",
        "                        self.positions[step + 1, i] = self.trade_env.position[0].item()\n",
        "\n",
        "                        # Update cash and BTC positions (use first simulation)\n",
        "                        current_price = self.trade_env.price_ary[self.trade_env.step_is[0] + self.trade_env.step_i, 2].item()\n",
        "                        self.cash[step + 1, i] = self.trade_env.cash[0].item()\n",
        "                        self.btc_positions[step + 1, i] = self.trade_env.position[0].item() * current_price\n",
        "\n",
        "                        # Debug: Log unusual asset values\n",
        "                        if step % 100 == 0:  # Only log every 100 steps to avoid spam\n",
        "                            print(f\"🔍 Debug: Step {step}, Agent {i}: Asset=${current_asset:.2f}\")\n",
        "\n",
        "                    # Store midpoint price (use first simulation)\n",
        "                    current_price = self.trade_env.price_ary[self.trade_env.step_is[0] + self.trade_env.step_i, 2].item()\n",
        "                    self.midpoints.append(current_price)\n",
        "\n",
        "                    # Prediction accuracy tracking (was only used for win/loss rates)\n",
        "\n",
        "                # Move to next step\n",
        "                step += 1\n",
        "                state = next_state\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Trading loop stopped at step {step}: {e}\")\n",
        "            # Truncate arrays to actual steps completed\n",
        "            actual_steps = max(1, step + 1)\n",
        "            print(f\"Truncating arrays to {actual_steps} steps\")\n",
        "\n",
        "            if actual_steps > 0 and self.net_assets.shape[1] > 0:\n",
        "                self.net_assets = self.net_assets[:actual_steps, :]\n",
        "                self.positions = self.positions[:actual_steps, :]\n",
        "                self.cash = self.cash[:actual_steps, :]\n",
        "                self.btc_positions = self.btc_positions[:actual_steps, :]\n",
        "                print(f\"Debug: Arrays truncated to shapes - net_assets: {self.net_assets.shape}\")\n",
        "            else:\n",
        "                print(f\"Error: Invalid array shapes after truncation\")\n",
        "                raise ValueError(\"Array truncation failed - invalid shapes\")\n",
        "\n",
        "        # Debug: Check array shapes after trading loop\n",
        "        print(f\"Debug: After trading loop - Array shapes:\")\n",
        "        print(f\"   net_assets: {self.net_assets.shape}\")\n",
        "        print(f\"   positions: {self.positions.shape}\")\n",
        "        print(f\"   cash: {self.cash.shape}\")\n",
        "        print(f\"   btc_positions: {self.btc_positions.shape}\")\n",
        "        print(f\"   midpoints length: {len(self.midpoints)}\")\n",
        "\n",
        "        # Debug: Check final asset values\n",
        "        print(f\"Debug: Final net_assets[-1]: {self.net_assets[-1] if len(self.net_assets) > 0 else 'No data'}\")\n",
        "        print(f\"Debug: Final cash[-1]: {self.cash[-1] if len(self.cash) > 0 else 'No data'}\")\n",
        "        print(f\"Debug: Final btc_positions[-1]: {self.btc_positions[-1] if len(self.btc_positions) > 0 else 'No data'}\")\n",
        "\n",
        "        # Calculate cumulative returns\n",
        "        self.cum_returns = self.net_assets / self.starting_cash\n",
        "        print(f\"Debug: cum_returns shape: {self.cum_returns.shape}\")\n",
        "        print(f\"Debug: Final cum_returns[-1]: {self.cum_returns[-1] if len(self.cum_returns) > 0 else 'No data'}\")\n",
        "\n",
        "        # Save results\n",
        "        results_dir = f\"{self.save_path}_evaluation_results\"\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "        # Save arrays\n",
        "        np.save(f\"{results_dir}/net_assets.npy\", self.net_assets)\n",
        "        np.save(f\"{results_dir}/positions.npy\", self.positions)\n",
        "        np.save(f\"{results_dir}/cash.npy\", self.cash)\n",
        "        np.save(f\"{results_dir}/btc_positions.npy\", self.btc_positions)\n",
        "        np.save(f\"{results_dir}/midpoints.npy\", np.array(self.midpoints))\n",
        "        np.save(f\"{results_dir}/cum_returns.npy\", self.cum_returns)\n",
        "\n",
        "        # Calculate and return metrics\n",
        "        return self.calculate_metrics(results_dir)\n",
        "\n",
        "    def calculate_metrics(self, results_dir: str) -> dict:\n",
        "        \"\"\"Calculate evaluation metrics with improved safety\"\"\"\n",
        "        # Debug: Check array shapes at start of calculate_metrics\n",
        "        print(f\"   Debug: In calculate_metrics - Array shapes:\")\n",
        "        print(f\"   net_assets: {self.net_assets.shape}\")\n",
        "        print(f\"   positions: {self.positions.shape}\")\n",
        "        print(f\"   cash: {self.cash.shape}\")\n",
        "        print(f\"   btc_positions: {self.btc_positions.shape}\")\n",
        "\n",
        "        # Check if we have enough data\n",
        "        if len(self.net_assets) < 2:\n",
        "            print(\"Error: Not enough data points for metrics calculation\")\n",
        "            return None\n",
        "\n",
        "        # Compute metrics with improved safety - MATCHING TASK1_EVAL.PY\n",
        "        print(f\"Debug: Before np.diff - net_assets shape: {self.net_assets.shape}\")\n",
        "\n",
        "        # Safe array operations with explicit copies\n",
        "        net_assets_diff = np.diff(self.net_assets, axis=0).copy()\n",
        "        net_assets_prev = self.net_assets[:-1].copy()\n",
        "\n",
        "        print(f\"Debug: net_assets_diff shape: {net_assets_diff.shape}\")\n",
        "        print(f\"Debug: net_assets_prev shape: {net_assets_prev.shape}\")\n",
        "\n",
        "        # Safe division with explicit shapes - handle division by zero\n",
        "        net_assets_prev_safe = np.where(net_assets_prev == 0, 1e-8, net_assets_prev)\n",
        "        returns = net_assets_diff / net_assets_prev_safe\n",
        "\n",
        "        # Handle any remaining invalid values (inf, nan)\n",
        "        returns = np.nan_to_num(returns, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        print(f\"Debug: After safe division - returns shape: {returns.shape}\")\n",
        "        print(f\"Debug: Returns range: min={returns.min():.6f}, max={returns.max():.6f}\")\n",
        "\n",
        "        # Safety check: Verify array shapes are valid\n",
        "        if returns.shape[0] <= 0 or returns.shape[1] <= 0:\n",
        "            print(f\"Error: Invalid returns array shape: {returns.shape}\")\n",
        "            return None\n",
        "\n",
        "        # Check if returns array is valid\n",
        "        if len(returns) == 0:\n",
        "            print(\"Error: No returns data available for metrics calculation\")\n",
        "            return None\n",
        "\n",
        "        final_sharpe_ratio = sharpe_ratio(returns)\n",
        "        final_max_drawdown = max_drawdown(returns)\n",
        "        final_roma = return_over_max_drawdown(returns)\n",
        "\n",
        "        # Win rate and loss rate calculations (not focusing on these metrics)\n",
        "\n",
        "        # Extract scalar values for calculations\n",
        "        final_net_assets = float(self.net_assets[-1, 0])\n",
        "        total_return = (final_net_assets - self.starting_cash) / self.starting_cash\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"EVALUATION RESULTS - {self.timeframe.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\" Starting Cash: ${self.starting_cash:,.2f}\")\n",
        "        print(f\" Final Net Assets: ${final_net_assets:,.2f}\")\n",
        "        print(f\" Total Return: {total_return:.4f} ({total_return*100:.2f}%)\")\n",
        "        print(f\" Sharpe Ratio: {final_sharpe_ratio:.4f}\")\n",
        "        print(f\" Max Drawdown: {final_max_drawdown:.4f}\")\n",
        "        print(f\" Return over Max Drawdown: {final_roma:.4f}\")\n",
        "        print(f\" Ensemble Size: {len(self.agents)} agents\")\n",
        "        print(f\" Results saved to: {results_dir}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Save metrics summary\n",
        "        metrics_summary = {\n",
        "            'timeframe': self.timeframe,\n",
        "            'starting_cash': float(self.starting_cash),\n",
        "            'final_net_assets': float(self.net_assets[-1, 0]),\n",
        "            'total_return': float(total_return),\n",
        "            'sharpe_ratio': float(final_sharpe_ratio),\n",
        "            'max_drawdown': float(final_max_drawdown),\n",
        "            'return_over_max_drawdown': float(final_roma),\n",
        "            'ensemble_size': int(len(self.agents)),\n",
        "            'agent_classes': [agent.__class__.__name__ for agent in self.agents]\n",
        "        }\n",
        "\n",
        "        with open(f\"{results_dir}/metrics_summary.json\", 'w') as f:\n",
        "            json.dump(metrics_summary, f, indent=2)\n",
        "\n",
        "        return metrics_summary\n",
        "\n",
        "    def plot_results(self, results_dir: str):\n",
        "        \"\"\"Plot evaluation results\"\"\"\n",
        "        try:\n",
        "            net_assets = np.load(f\"{results_dir}/net_assets.npy\")\n",
        "            positions = np.load(f\"{results_dir}/positions.npy\")\n",
        "            cash = np.load(f\"{results_dir}/cash.npy\")\n",
        "            btc_positions = np.load(f\"{results_dir}/btc_positions.npy\")\n",
        "\n",
        "            # Debug: Check array shapes after loading\n",
        "            print(f\"   Debug: Plotting - Array shapes after loading:\")\n",
        "            print(f\"   net_assets: {net_assets.shape}\")\n",
        "            print(f\"   positions: {positions.shape}\")\n",
        "            print(f\"   cash: {cash.shape}\")\n",
        "            print(f\"   btc_positions: {btc_positions.shape}\")\n",
        "\n",
        "            # Optional arrays\n",
        "            midpoints = None\n",
        "            cum_returns = None\n",
        "            midpoints_path = f\"{results_dir}/midpoints.npy\"\n",
        "            cum_returns_path = f\"{results_dir}/cum_returns.npy\"\n",
        "            if os.path.exists(midpoints_path):\n",
        "                midpoints = np.load(midpoints_path)\n",
        "            if os.path.exists(cum_returns_path):\n",
        "                cum_returns = np.load(cum_returns_path)\n",
        "\n",
        "            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "            # Net Assets over time\n",
        "            ax1.plot(net_assets, label='Net Assets', color='blue')\n",
        "            ax1.set_title('Net Assets Over Time')\n",
        "            ax1.set_xlabel('Steps')\n",
        "            ax1.set_ylabel('Net Assets ($)')\n",
        "            ax1.grid(True)\n",
        "            ax1.legend()\n",
        "\n",
        "            # Cash and BTC positions\n",
        "            ax2.plot(cash, label='Cash', color='green', alpha=0.7)\n",
        "            ax2.plot(btc_positions, label='BTC Value', color='orange', alpha=0.7)\n",
        "            ax2.set_title('Cash vs BTC Positions')\n",
        "            ax2.set_xlabel('Steps')\n",
        "            ax2.set_ylabel('Value ($)')\n",
        "            ax2.grid(True)\n",
        "            ax2.legend()\n",
        "\n",
        "            # Trading positions\n",
        "            ax3.plot(positions, label='Position', color='red')\n",
        "            ax3.set_title('Trading Positions')\n",
        "            ax3.set_xlabel('Steps')\n",
        "            ax3.set_ylabel('Position')\n",
        "            ax3.grid(True)\n",
        "            ax3.legend()\n",
        "\n",
        "            # Returns distribution\n",
        "            if net_assets.shape[1] > 0:  # Check if array has valid shape\n",
        "                net_assets_diff = np.diff(net_assets, axis=0).copy()\n",
        "                net_assets_prev = net_assets[:-1].copy()\n",
        "                # Safe division\n",
        "                net_assets_prev_safe = np.where(net_assets_prev == 0, 1e-8, net_assets_prev)\n",
        "                returns = net_assets_diff / net_assets_prev_safe\n",
        "                returns = np.nan_to_num(returns, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "                ax4.hist(returns.flatten(), bins=50, alpha=0.7, color='purple')\n",
        "                ax4.set_title('Returns Distribution')\n",
        "                ax4.set_xlabel('Returns')\n",
        "                ax4.set_ylabel('Frequency')\n",
        "                ax4.grid(True)\n",
        "            else:\n",
        "                ax4.text(0.5, 0.5, 'No valid returns data', ha='center', va='center', transform=ax4.transAxes)\n",
        "                ax4.set_title('Returns Distribution')\n",
        "                ax4.set_xlabel('Returns')\n",
        "                ax4.set_ylabel('Frequency')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{results_dir}/evaluation_plots.png\", dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            print(f\"Plots saved to: {results_dir}/evaluation_plots.png\")\n",
        "\n",
        "            # Additional plot: Cumulative returns vs Midpoint over timesteps\n",
        "            print(f\"🔍 Debug: Plotting cumulative returns vs midpoint:\")\n",
        "            print(f\"   cum_returns shape: {cum_returns.shape if cum_returns is not None else 'None'}\")\n",
        "            print(f\"   midpoints length: {len(midpoints) if midpoints is not None else 'None'}\")\n",
        "\n",
        "            if cum_returns is None:\n",
        "                cum_returns = net_assets / net_assets[0]\n",
        "                print(f\"   Created cum_returns with shape: {cum_returns.shape}\")\n",
        "\n",
        "            # Align lengths: cum_returns includes initial point; midpoints collected per step\n",
        "            if midpoints is not None and len(midpoints) > 0:\n",
        "                n = min(len(midpoints), max(0, len(cum_returns) - 1))\n",
        "                print(f\"   Aligned length n: {n}\")\n",
        "\n",
        "                if n > 0:\n",
        "                    x = np.arange(n)\n",
        "                    cr_plot = cum_returns[1:1 + n]\n",
        "                    mp_plot = midpoints[:n]\n",
        "\n",
        "                    print(f\"   cr_plot shape: {cr_plot.shape}, mp_plot length: {len(mp_plot)}\")\n",
        "\n",
        "                    fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "                    color1 = 'tab:blue'\n",
        "                    ax1.set_xlabel('Timesteps')\n",
        "                    ax1.set_ylabel('Cumulative Return (x)', color=color1)\n",
        "\n",
        "                    # Handle single data point case\n",
        "                    if n == 1:\n",
        "                        # For single point, plot as scatter points to make them visible\n",
        "                        ax1.scatter(x, cr_plot, color=color1, s=100, label='Cumulative Return', zorder=5)\n",
        "                        ax1.axhline(y=cr_plot[0], color=color1, alpha=0.3, linestyle='--')\n",
        "                        print(f\"   Single data point detected - using scatter plot for visibility\")\n",
        "                    else:\n",
        "                        # For multiple points, use line plot\n",
        "                        ax1.plot(x, cr_plot, color=color1, label='Cumulative Return')\n",
        "\n",
        "                    ax1.tick_params(axis='y', labelcolor=color1)\n",
        "                    ax1.grid(True, which='both', axis='both', alpha=0.3)\n",
        "\n",
        "                    ax2 = ax1.twinx()\n",
        "                    color2 = 'tab:orange'\n",
        "                    ax2.set_ylabel('Midpoint Price', color=color2)\n",
        "\n",
        "                    # Handle single data point case for midpoint\n",
        "                    if n == 1:\n",
        "                        ax2.scatter(x, mp_plot, color=color2, s=100, alpha=0.8, label='Midpoint', zorder=5)\n",
        "                        ax2.axhline(y=mp_plot[0], color=color2, alpha=0.3, linestyle='--')\n",
        "                    else:\n",
        "                        ax2.plot(x, mp_plot, color=color2, alpha=0.6, label='Midpoint')\n",
        "\n",
        "                    ax2.tick_params(axis='y', labelcolor=color2)\n",
        "\n",
        "                    plt.title(f'Cumulative Returns and Midpoint over Timesteps - {self.timeframe}')\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(f\"{results_dir}/cum_returns_vs_midpoint.png\", dpi=300, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "                    print(f\"Saved: {results_dir}/cum_returns_vs_midpoint.png\")\n",
        "                else:\n",
        "                    print(f\"Warning: No valid data points for cumulative returns plot (n={n})\")\n",
        "            else:\n",
        "                print(f\"   Warning: No midpoints data available for cumulative returns plot\")\n",
        "                print(f\"   midpoints: {midpoints}\")\n",
        "\n",
        "            # Add note about limited data for higher timeframes\n",
        "            if len(midpoints) <= 5:\n",
        "                print(f\"   Note: Very limited data points ({len(midpoints)}) for {self.timeframe} timeframe\")\n",
        "                print(f\"   This suggests the trading environment may need configuration adjustments\")\n",
        "                print(f\"   Consider: increasing seq_len, checking data availability, or using lower timeframes\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Failed to create plots: {str(e)}\")\n",
        "\n",
        "# ==================== MAIN EVALUATION FUNCTIONS ====================\n",
        "\n",
        "def evaluate_improved_ensemble(save_path: str, agent_classes: List, timeframe: str = '1sec', gpu_id: int = 0):\n",
        "    \"\"\"Main evaluation function\"\"\"\n",
        "    print(f\"  Starting PPO Ensemble Evaluation - {timeframe}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Check if trained agents exist\n",
        "    if not os.path.exists(save_path):\n",
        "        print(f\"  Error: Trained agents directory not found: {save_path}\")\n",
        "        print(f\"  Please ensure you have trained PPO agents.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Initialize evaluator\n",
        "        evaluator = ImprovedEnsembleEvaluator(\n",
        "            save_path=save_path,\n",
        "            agent_classes=agent_classes,\n",
        "            timeframe=timeframe,\n",
        "            gpu_id=gpu_id\n",
        "        )\n",
        "\n",
        "        # Load agents\n",
        "        evaluator.load_agents()\n",
        "\n",
        "        # Run evaluation\n",
        "        metrics = evaluator.multi_trade()\n",
        "\n",
        "        # Create plots after evaluation\n",
        "        if metrics:\n",
        "            results_dir = f\"{save_path}_evaluation_results\"\n",
        "            evaluator.plot_results(results_dir)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Evaluation failed: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCkqN8KvnlUt",
        "outputId": "50b0760f-a520-469c-cc3d-3a0761c7ea7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Improved Ensemble Evaluation Script\n",
            "📁 Save Path: ./trained_agents\n",
            "⏰ Timeframe: 1sec\n",
            "🖥️  GPU ID: -1\n",
            "🤖 Agents: ['ImprovedPPO']\n",
            "📊 Data Split: VALID\n",
            "🔧 LLM Normalization: ✅ Enabled\n",
            "🎯 Expected Agent Path: ./trained_agents/5_7_3/1sec/PPO\n",
            "🚀 Starting IMPROVED PPO Ensemble Evaluation - 1sec\n",
            "============================================================\n",
            "✅ Found valid data at: output/4_1/1sec/valid_predictions.npy\n",
            "✅ Loaded valid data: torch.Size([74212, 8])\n",
            "✅ Found price data at: data/1sec/BTC_1sec_with_sentiment_risk_train_1sec_train_70.csv\n",
            "🔍 Environment initialized:\n",
            "   - max_step: 870\n",
            "   - seq_len: 1800\n",
            "   - full_seq_len: 74212\n",
            "   - state_dim: 12\n",
            "   - action_dim: 3\n",
            "🔍 Loading agents from: ./trained_agents/5_7_3/1sec/PPO\n",
            "✅ Loaded Improved PPO agent from ./trained_agents/5_7_3/1sec/PPO\n",
            "✅ Loaded Improved PPO agent from ./trained_agents/5_7_3/1sec/PPO\n",
            "🤖 Loaded 1 agents successfully\n",
            "🔍 Agent details:\n",
            "   Agent 0: ImprovedPPO\n",
            "   State dim: 12\n",
            "   Action dim: 3\n",
            "🚀 Starting improved multi-agent trading simulation...\n",
            "🔍 Debug: Environment reset - Cash: 1000000.00, Asset: 1000000.00\n",
            "🔍 Debug: num_steps=870, num_agents=1\n",
            "🔍 Debug: Creating arrays with shape (870, 1)\n",
            "🔍 Debug: Array shapes - net_assets: (870, 1), positions: (870, 1)\n",
            "🔍 Debug: Initial setup complete. Starting cash: $1000000.0\n",
            "🔍 Debug: Net assets[0]: [1000000.]\n",
            "🔍 Debug: Step 0, Agent 0: Asset=$1000000.00\n",
            "🔍 Debug: Step 100 - Old Asset: 1000000.00, New Asset: 1000000.00, Reward: 0.000000\n",
            "🔍 Debug: Step 200 - Old Asset: 1000000.00, New Asset: 1000000.00, Reward: 0.000000\n",
            "🔍 Debug: Step 100, Agent 0: Asset=$1000000.00\n",
            "🔍 Debug: Step 300 - Old Asset: 1000000.00, New Asset: 1000000.00, Reward: 0.000000\n",
            "🔍 Debug: Step 400 - Old Asset: 1000000.00, New Asset: 1000000.00, Reward: 0.000000\n",
            "🔍 Debug: Step 200, Agent 0: Asset=$1000000.00\n",
            "🔍 Debug: Step 500 - Old Asset: 1000000.00, New Asset: 1000000.00, Reward: 0.000000\n",
            "🔍 Debug: Step 600 - Old Asset: 1000028.56, New Asset: 1000028.56, Reward: 0.000000\n",
            "🔍 Debug: Step 300, Agent 0: Asset=$1000028.56\n",
            "🔍 Debug: Step 700 - Old Asset: 1000169.38, New Asset: 1000169.38, Reward: 0.000000\n",
            "🔍 Debug: Step 800 - Old Asset: 1000169.38, New Asset: 1000169.38, Reward: 0.000000\n",
            "🔍 Debug: Step 400, Agent 0: Asset=$1000169.38\n",
            "🔍 Debug: Step 900 - Old Asset: 1000169.38, New Asset: 1000169.38, Reward: 0.000000\n",
            "🔍 Debug: Step 1000 - Old Asset: 1000169.38, New Asset: 1000169.38, Reward: 0.000000\n",
            "🔍 Debug: Step 500, Agent 0: Asset=$1000169.38\n",
            "🔍 Debug: Step 1100 - Old Asset: 1000169.38, New Asset: 1000169.38, Reward: 0.000000\n",
            "🔍 Debug: Step 1200 - Old Asset: 1000169.38, New Asset: 1000169.38, Reward: 0.000000\n",
            "🔍 Debug: Step 600, Agent 0: Asset=$1000169.38\n",
            "🔍 Debug: Step 1300 - Old Asset: 1000169.38, New Asset: 1000169.38, Reward: 0.000000\n",
            "🔍 Debug: Step 1400 - Old Asset: 1000169.38, New Asset: 1000169.38, Reward: 0.000000\n",
            "🔍 Debug: Step 700, Agent 0: Asset=$1000169.38\n",
            "🔍 Debug: Step 1500 - Old Asset: 1000169.38, New Asset: 1000169.38, Reward: 0.000000\n",
            "🔍 Debug: Step 1600 - Old Asset: 1000169.38, New Asset: 1000169.38, Reward: 0.000000\n",
            "🔍 Debug: Step 800, Agent 0: Asset=$1000169.38\n",
            "🔍 Debug: Step 1700 - Old Asset: 1000169.38, New Asset: 1000169.38, Reward: 0.000000\n",
            "🔍 Debug: After trading loop - Array shapes:\n",
            "   net_assets: (870, 1)\n",
            "   positions: (870, 1)\n",
            "   cash: (870, 1)\n",
            "   btc_positions: (870, 1)\n",
            "   midpoints length: 869\n",
            "🔍 Debug: Final net_assets[-1]: [1000169.375]\n",
            "🔍 Debug: Final cash[-1]: [1000169.375]\n",
            "🔍 Debug: Final btc_positions[-1]: [0.]\n",
            "🔍 Debug: cum_returns shape: (870, 1)\n",
            "🔍 Debug: Final cum_returns[-1]: [1.00016938]\n",
            "🔍 Debug: In calculate_metrics - Array shapes:\n",
            "   net_assets: (870, 1)\n",
            "   positions: (870, 1)\n",
            "   cash: (870, 1)\n",
            "   btc_positions: (870, 1)\n",
            "🔍 Debug: Before np.diff - net_assets shape: (870, 1)\n",
            "🔍 Debug: net_assets_diff shape: (869, 1)\n",
            "🔍 Debug: net_assets_prev shape: (869, 1)\n",
            "🔍 Debug: After safe division - returns shape: (869, 1)\n",
            "🔍 Debug: Returns range: min=-0.000000, max=0.000046\n",
            "\n",
            "============================================================\n",
            "📊 IMPROVED EVALUATION RESULTS - 1SEC\n",
            "============================================================\n",
            "💰 Starting Cash: $1,000,000.00\n",
            "💰 Final Net Assets: $1,000,169.38\n",
            "📈 Total Return: 0.0002 (0.02%)\n",
            "📊 Sharpe Ratio: 0.0882\n",
            "📉 Max Drawdown: -0.0000\n",
            "📈 Return over Max Drawdown: 903.4865\n",
            "🤖 Ensemble Size: 1 agents\n",
            "📁 Results saved to: ./trained_agents/5_7_3/1sec/PPO_evaluation_results\n",
            "============================================================\n",
            "🔍 Debug: Plotting - Array shapes after loading:\n",
            "   net_assets: (870, 1)\n",
            "   positions: (870, 1)\n",
            "   cash: (870, 1)\n",
            "   btc_positions: (870, 1)\n",
            "📊 Plots saved to: ./trained_agents/5_7_3/1sec/PPO_evaluation_results/evaluation_plots.png\n",
            "🔍 Debug: Plotting cumulative returns vs midpoint:\n",
            "   cum_returns shape: (870, 1)\n",
            "   midpoints length: 869\n",
            "   Aligned length n: 869\n",
            "   cr_plot shape: (869, 1), mp_plot length: 869\n",
            "📈 Saved: ./trained_agents/5_7_3/1sec/PPO_evaluation_results/cum_returns_vs_midpoint.png\n",
            "🎉 Improved PPO evaluation completed!\n"
          ]
        }
      ],
      "source": [
        "# Default parameters\n",
        "save_path = \"./trained_agents\"\n",
        "agent_classes = [ImprovedPPO]\n",
        "gpu_id = -1\n",
        "timeframe = \"1sec\"\n",
        "\n",
        "print(f\" Improved Ensemble Evaluation Script\")\n",
        "print(f\" Save Path: {save_path}\")\n",
        "print(f\" Timeframe: {timeframe}\")\n",
        "print(f\"  GPU ID: {gpu_id}\")\n",
        "print(f\" Agents: {[cls.__name__ for cls in agent_classes]}\")\n",
        "print(f\" Data Split: {EVAL_DATA_SPLIT.upper()}\")\n",
        "print(f\" LLM Normalization: {'Enabled' if NORMALIZE_LLM_SIGNALS else 'Disabled'}\")\n",
        "print(f\" Expected Agent Path: {save_path}/{PPO_VERSION}/{timeframe}/{AGENT_DIR_NAME}\")\n",
        "\n",
        "# Check if agents exist for specific timeframe under versioned directory\n",
        "timeframe_save_path = f\"{save_path}/{PPO_VERSION}/{timeframe}/{AGENT_DIR_NAME}\"\n",
        "if not os.path.exists(timeframe_save_path):\n",
        "    print(f\"   No trained agents found for {timeframe}\")\n",
        "    print(f\"   Expected path: {timeframe_save_path}\")\n",
        "    print(f\"   Please train agents first using the improved training script!\")\n",
        "else:\n",
        "    evaluate_improved_ensemble(timeframe_save_path, agent_classes, timeframe, gpu_id)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
